import numpy as np
# import results.plots as lplot
import matplotlib.pyplot as plt


# x = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
# y_mf = [1.067420797783607, 1.00781605094093, 0.98438877485892695, 0.96875040306964932, 0.95776595779752793, 0.9525320859365054, 0.94193699932605024, 0.94017977655057627, 0.94096263857607743, 0.92878159918291792, 0.92916187509905335]
# y_cs = [1.0218783274248637, 0.98275104265706714, 0.96513355361042175, 0.95476157192768984, 0.94953346297267871, 0.94440433834307558, 0.93973616632236223, 0.9402632776631612, 0.9409159936389867, 0.93649854759654028, 0.93490067054358628]

# Using Adadelta initial training
x = np.array([0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60])
y_mf = np.array([1.0724185984016452, 0.99819044711689253, 0.96772166640275337, 0.95332651184846462, 0.94288815755938415, 0.93465798231530006, 0.92614098916314291, 0.92250781062029175, 0.91849448639890963, 0.91590734085709624, 0.91183403211203962, 0.911173320381507, 0.90556936854912462])
# y_cs = np.array([1.022406356734737, 0.99516120381487061, 0.97740903209312069, 0.97316486760266296, 0.95655580602109391, 0.95275242611112965, 0.94390055158442199, 0.94374085225589821, 0.94037538225113015, 0.94201519696465741, 0.93797269222673452, 0.93892431236305429, 0.93391972042105675])

# Using Nadam
y_cs = np.array([1.0218783274248637, 0.98275104265706714, 0.96513355361042175, 0.95476157192768984, 0.94953346297267871, 0.94440433834307558, 0.93973616632236223, 0.9402632776631612, 0.9409159936389867, 0.93649854759654028, 0.93490067054358628, 0.93363299693411173, 0.9307114196283283])

y_mf_baseline = np.array([1.0224901253238088, 0.98380895504266441, 0.96240847664167495, 0.95565298016613032, 0.94992969443044795, 0.9432740620636747, 0.94270414021123605, 0.93915310777940841, 0.93846756804429066, 0.93736807962184976, 0.93595355800664892, 0.93429297603514216, 0.93398438595959743])

# Whole hybrid model
y_mf_x = np.array([1.0252752458584815, 0.97841850087380389, 0.96036753756658433, 0.94748478578633355, 0.93768332500613849, 0.92973528549840823, 0.92646737448326644, 0.91838643641610607, 0.91609335677696402, 0.91154611434872845, 0.91153957751211534, 0.90640222813771509, 0.90013350021210448])
y_cs_x = np.array([1.0206021957970846, 0.97584528265888126, 0.96219607340754532, 0.9546447915487446, 0.94793476780495423, 0.94382605670261621, 0.94235898511716021, 0.93813339794804618, 0.93648804886104775, 0.93438192836754064, 0.93522182250587194, 0.93268341688222844, 0.92878841018332425])

# Bias Baseline
y_bias_baseline = np.array([1.0220813093380368, 0.98428909078493665, 0.97040356694732599, 0.96691624031807177, 0.952486950082856, 0.95113602934906849, 0.95001422944870795, 0.94832274604191602, 0.94228636554701106, 0.94172117450720083, 0.94263505350980137, 0.93953480459975147, 0.93986048894759366])

y_hybrid = []

for y1, y2 in zip(y_mf_x, y_cs_x):
    y_hybrid.append(min(y1, y2))


# plt.plot(x, y_bias_baseline-y_cs, 'r-')
# plt.plot(x, y_bias_baseline-y_cs_x, 'g-')
# plt.plot(x, y_bias_baseline-y_mf_x, 'b-')
plt.plot(x, y_mf_baseline-y_cs_x)
plt.show()


# fig, ax = lplot.newfig(0.9)
#
# # plt.style.use('acm-1col')
# ax.plot(x, y_mf_baseline, 'r-', label='MF')
# ax.plot(x, y_cs, 'b--', label='CbF/DF')
# # ax.plot(x, y_hybrid, 'g:', label='Hybrid')
# ax.set_xlabel('Ratings / User')
# ax.set_ylabel('RMSE')
# ax.legend()
# lplot.savefig('ratings_per_user')
# # plt.show()
